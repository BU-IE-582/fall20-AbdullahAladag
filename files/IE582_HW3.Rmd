---
title: "HOMEWORK 3"
author: "Abdullah AladaÄŸ - IE582 - Fall 2020"
due: January 1
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}
# Import library
setwd("C:/Users/asus/desktop")
library(glmnet)
library(Metrics)
library(dplyr)

```


## Task A

In this part, our aim is to create lag168 and lag 48 in order to predict consumption of electricity. After predicting consumption of electricity, we asses accuracy of them. Therefore, mean absolute percentage error is calculated for lag 168 and lag 48

```{r}

# Read data
setwd("C:/Users/asus/desktop")
all_data = read.table("Tuketim.csv", sep = ",")
names(all_data)[1] = "Date"
names(all_data)[2] = "Hour"
names(all_data)[3] = "Cons"
all_data <- all_data[-1,]
all_data$Cons = as.numeric(gsub(",","",all_data$Cons))*1000
```


```{r}
# Create Lag 48 and Lag 168
data <- all_data[169:nrow(all_data),]
data$LAG168 <- all_data$Cons[1:(nrow(all_data)-168)]
data$LAG48  <- all_data$Cons[121:(nrow(all_data)-48)]

test_nov_20 <- data[(42386-169):nrow(data),]

```


```{r}
# Calculate mape for Lag 168 and Lag 48
mape_lag168 = mape(test_nov_20$Cons,test_nov_20$LAG168)*100
mape_lag48= mape(test_nov_20$Cons,test_nov_20$LAG48)*100

print(paste("MAPE for Lag168 :",mape_lag168))
print(paste("MAPE for Lag 48 :",mape_lag48))
```

MAPE value for lag168 is equal to 3.5 % and MAPE value for lag48 is equal to 8 % . This means that prediction accuracy is higher when lag148 is used for predicting electricity consumption.


## Task B

Our aim is to model certain data which is until 1 Nov. 2020 and then to test data to evaluate the model accuracy. For modelling purposes, Linear regression is used to model data. After modelling certain data, we predict some electiricty consumption belonging to some days which is between 1 Nov 2020 and 01 Dec 2020.

```{r}
train_data = data[1:42216,]
features_train = train_data[,3:5]

test_data = data[42217:nrow(data),]
features_test = test_data[,3:5]

fit= lm(Cons~.,features_train)
summary(fit)



```

Residual is the actual value - predicted value.When looking at summary statistics of the model, we can realize that some consumption of electricity are predicted lower than actual value which is around 16 MW and some of ones are predicted higher than actual value which is around 25 MW. Three features including intercept are found to be significant according to its p value.

```{r}

prediction = predict(fit,features_test)

plot(test_data$Cons,prediction, xlab = "Actual value of consumption", ylab = "Predicted value of consuption") 
abline(a=0,b=1,col=2)  

mape_prediction = mape(test_data$Cons,prediction)* 100
print(mape_prediction)
```

Mape value for test data is around 4 % which means that prediction accuracy is high. In addtion to this, it is possible to make evaluations in terms of a particular consumption point looking at graph which is actual value vs predicted value. In general, data fits to model but some consumption values higher than 40 MW was predicted as lower than actual value by model.

## Task C

```{r}
hours = c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00","10:00",
          "11:00","12:00","13:00","14:00","15:00","16:00","17:00","18:00","19:00","20:00","21:00"
          ,"22:00","23:00")

mape_1 = rep(0,24)
summary_1 = matrix(rep(0,72),24,3)

for(i in 1:24){

  train_data_i = train_data %>% filter(Hour == hours[i])
  features_train_i = train_data_i[,3:5]
  fit_i = lm(Cons~.,features_train_i)
  summary_1[i,] = fit_i$coefficients
  
  test_data_i = test_data %>% filter(Hour == hours[i])
  features_test_i = test_data_i[,3:5]
  predicted = predict(fit_i,test_data_i)
  
  act_val = test_data %>% filter(Hour == hours[i])
  mape_1[i] = mape(predicted,act_val[,3])*100
  print(paste("Mape Value (%)  for ",hours[i],":",mape_1[i] ))
  
}

summary_1 = as.data.frame(summary_1)
names(summary_1)[1] = " Intercept"
names(summary_1)[2] = "LAG168"
names(summary_1)[3] = "LAG48"
print(summary_1)
plot(1:24,mape_1,lwd = 2, type = "l",xlab = "Hour", ylab = "MAPE",main = "Mape Value vs Hour" )
```

Every hours were modelled separately using whole training data set.In order to model train data, intercept, lag 48 and lag 168 were used as features.In many cases, intercept is useful because it tries to model the noise.The effects that cannot be explained will be modeled in the intercept part.After modelling part,test data was used to determine prediction accuracy of this model. Mape values for certain hours which is first and last hours of the day was calculated lower than other hours of the day. This means that prediction accuracy for certain hours is higher than middle hours of the day.


## Task D
```{r,warning= FALSE}
wide_format_features_train <- reshape(train_data, timevar = "Hour", idvar = "Date", drop = c("Cons"),direction = "wide")
wide_format_target_train <- reshape(train_data[,1:3], timevar = "Hour", idvar ="Date", direction = "wide")
wide_train = cbind(wide_format_features_train,wide_format_target_train[,-1] )
wide_train = na.omit(wide_train)

wide_feat_test <- reshape(test_data, timevar = "Hour", idvar = "Date", drop = "Cons", direction = "wide")
wide_targ_test <- reshape(test_data[,1:3], timevar = "Hour", idvar = "Date", direction = "wide" )


mape_2 = rep(0, times=24)
set.seed(1010)

summary_2 = matrix(rep(0,72), 24, 3)
for(i in 1:24){

  
glmmod = cv.glmnet(as.matrix(wide_train[,2:49]),wide_train[,(i+49)],nfolds= 10,family = "gaussian")

lass = glmnet(as.matrix(wide_train[,2:49]),wide_train[,i+49],family = "gaussian",lambda = glmmod$lambda.min)

prediction = predict(lass,as.matrix(wide_feat_test[,2:ncol(wide_feat_test)]))

mape_cons <- mape(prediction,wide_targ_test[,(i+1)])*100

print(paste("Mape for",hours[i],mape_cons))

mape_2[i] <- mape_cons
}

plot(1:24,mape_2,type = "l", col = "blue", xlab = "Hour", ylab = "MAPE", lwd = 2,main = "Mape Value vs Hour") 
```

In this part, we use lasso regression in order to minimize objective function. With lasso regression, a budget on the coefficients is provided.In other words, we define lambda value to minimize objective function. First, cross-validation was used to define lambda value, and then lambda min which gives us minimum error was selected to perform lasso regression. Compared to part c, prediction accuracy for all hours was improved but graph which is mape value vs hours still has the same pattern with linear regression mape values graph.

## Task F

Boxplot was drawn in order to compare mape values. As seen in the boxplot, model devising penalized linear regression has lower mape values than the one using linear regression. 

```{r}
boxplot(mape_1,mape_2,col = c("green","blue"),names = c("Linear Regression","Lasso Regression"))
```




```{r}

```





```{r}

```




```{r}

```




```{r}

```




```{r}

```




```{r}

```
