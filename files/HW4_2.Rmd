---
title: "HOMEWORK 4"
author: "Abdullah AladaÄŸ - IE582 - Fall 2020"
due: January 30
output: html_document
---


# Mushroom Data Set

 This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom

### Features Information


1.cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s

2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s

3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y

4. bruises?: bruises=t,no=f

5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s

6. gill-attachment: attached=a,descending=d,free=f,notched=n

7. gill-spacing: close=c,crowded=w,distant=d

8. gill-size: broad=b,narrow=n

9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,
green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y

10. stalk-shape: enlarging=e,tapering=t

11. stalk-root: bulbous=b,club=c,cup=u,equal=e,
rhizomorphs=z,rooted=r,missing=?

12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s

13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s

14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y

15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y

16. veil-type: partial=p,universal=u

17. veil-color: brown=n,orange=o,white=w,yellow=y

18. ring-number: none=n,one=o,two=t

19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z

20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y

21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y

22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d


  This data set includes categorical features.In part of data manipulation, data set is converted into dummy variables for linear regression. Original data set was used by other learners.



```{r,warning=FALSE,message=FALSE}
library("caret")
library("dplyr")
library("glmnet")
library(Metrics)
require(rpart)
library(rattle)
library(RColorBrewer)
require(data.table,quietly = TRUE)
library(data.table)
library(randomForest)
library(gbm)
library(xgboost)
library(e1071)
```

## Data Manipulation Part

```{r}
setwd("C:/Users/asus/Desktop")

mush = read.table("agaricus-lepiota.data.csv",sep=",")

for(i in 1:ncol(mush)){
  
  mush[,i] = as.factor(mush[,i])
  
  
}

mush <- mush[,-17]

# convert categorical variables to dummy
Dummies =  dummyVars("~.", data = mush[,2:ncol(mush)],fullRank = T)
mush_dummy = data.frame(predict(Dummies, newdata = mush[,2:ncol(mush)]))

mush_dummy$pois <- mush$V1





# split whole data as training and test 

idx <- sample(seq(1, 2), size = nrow(mush_dummy), replace = TRUE, prob = c(.75, .25))

train_mush <- mush[idx == 1,]
test_mush <- mush[idx == 2,]

train_mush_dummy <- mush_dummy[idx == 1,]
test_mush_dummy <-  mush_dummy[idx == 2,]




```

## Penalized Linear Regression


```{r}
# apply penalized linear regression


# for train set
control <- trainControl(method = "repeatedcv",number = 5, repeats = 2, allowParallel = T)


PRA = function(lambda){

glmmod = glmnet(as.matrix(train_mush_dummy[,1:(ncol(train_mush_dummy)-1)]),train_mush_dummy$pois, family = "binomial",type.measure = "class",trcontrol = control)




# for test set


prediction_test = predict(glmmod,as.matrix(test_mush_dummy[,1:(ncol(test_mush_dummy)-1)]),type = "class",s = lambda)
test_error= confusionMatrix(as.factor(prediction_test),test_mush_dummy$pois)

train_prediction<- predict(glmmod,as.matrix(train_mush_dummy[,1:(ncol(train_mush_dummy)-1)]),type = "class",s = lambda)
error_train <- confusionMatrix(as.factor(train_prediction),train_mush_dummy$pois)


print("Confusion matrix PLR performing on test data ")
print(test_error$table)      
print(paste("Accuracy for test data", test_error$overall[1]))

print(paste("Accuracy for training data", error_train$overall[1]))

}


```


```{r}
PRA(0.01)
```



```{r}
PRA(0.001)

```



```{r}
PRA(0.005)
```

## Decision Tree

```{r}
# Classification Tree


Decision_tree <- function(cp,minsplit){

class_tree = rpart(V1~.,train_mush, method = "class",cp = cp, minsplit = minsplit)


test_prediction= predict(class_tree,test_mush, type = "class")
error_test <- confusionMatrix(test_prediction,test_mush$V1)

train_prediction<- predict(class_tree,train_mush,type = "class")
error_train <- confusionMatrix(as.factor(train_prediction),train_mush$V1)


print("Confusion matrix for DT performing on test data ")
print(error_test$table)      
print(paste("Accuracy for test data", error_test$overall[1]))

print(paste("Accuracy for training data", error_train$overall[1]))


}
```



```{r }
Decision_tree(cp = 0.01, minsplit =20)

```




```{r }
Decision_tree(cp = 0.001, minsplit =20)


```



```{r }
Decision_tree(cp = 0.001, minsplit = 10)


```


## Random Forest

```{r}
#Random forest

Random_forest <- function(numfeat){
RF = randomForest(train_mush[,2:ncol(train_mush)],train_mush$V1,type = "classification", mtry = numfeat )
print(RF)

# Test data
prediction_test = predict(RF,test_mush, type = "class")
error_test <- confusionMatrix(prediction_test,test_mush$V1)

train_prediction<- predict(RF,train_mush,type = "class")
error_train <- confusionMatrix(as.factor(train_prediction),train_mush$V1)



print("Confusion matrix for RF performing on test data ")
print(error_test$table)      
print(paste("Accuracy for test data", error_test$overall[1]))

print(paste("Accuracy for training data", error_train$overall[1]))

}
```


```{r }
Random_forest(5)

```



```{r }
Random_forest(10)


```



```{r }
Random_forest(15)


```


## Stochastic Gradient Boosting

```{r, warning= F}
# stochastic gradient boosting

control <- trainControl(method = "repeatedcv",number = 5, repeats = 2, allowParallel = T)
tunning  <- expand.grid(n.trees = c(100,150,200), interaction.depth=c(1:3), shrinkage=c(0.01,0.05,0.1), n.minobsinnode=c(20))
                                 
unwantedoutput =  capture.output(stochastic <- train(V1~., data = train_mush,trControl = control,method = "gbm",tuneGrid = tunning))
print(stochastic)  
  
  
prediction <- predict(stochastic,test_mush)
error_test <- confusionMatrix(as.factor(prediction),test_mush$V1)
  
train_prediction<- predict(stochastic,train_mush)
error_train <- confusionMatrix(as.factor(train_prediction),train_mush$V1)

error_train$table
error_train$overall
```

 There is no difference among test accuracy acquired when applying different learners. All learners had high accuracy  in terms of test and training. In addition to this,it is not possible to compare a learner hyperparameters. Therefore, This data set is not feasible to compare learners' performance on it. 


```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```



```{r}

```


```{r}

```



```{r}

```

```{r}

```
